# Slow Learning Strategy f√ºr Bessere R√§umliche Koh√§renz

## Problem

**Aktueller Status:**
- ‚úÖ Loss: 0.005 (exzellent)
- ‚úÖ Accuracy: 99.5% (sehr gut)
- ‚ùå R√§umliche Koh√§renz: schlecht (schwebende Teile, getrennte Strukturen)

**Ursache:**
Das Modell lernt **zu schnell** lokale Block-Muster, aber **zu langsam** globale r√§umliche Zusammenh√§nge.

## L√∂sung: Langsameres Lernen + L√§ngeres Training

### √Ñnderungen in `config/config.yaml`

| Parameter | Alt | Neu | Grund |
|-----------|-----|-----|-------|
| `learning_rate` | 0.0001 | **0.00005** | Halbiert ‚Üí mehr Zeit f√ºr r√§umliche Muster |
| `num_epochs` | 200 | **300** | +50% Training ‚Üí mehr Gelegenheit zu lernen |
| `warmup_epochs` | 5 | **10** | Sanfterer Start f√ºr stabiles langsames Lernen |
| `early_stopping.patience` | 50 | **75** | Mehr Geduld f√ºr langsamere Konvergenz |

### Warum das funktionieren sollte

**Theorie:**
1. **Schnelles Lernen** ‚Üí Modell perfektioniert lokale Muster (Block-Nachbarschaften)
2. **Langsames Lernen** ‚Üí Modell hat Zeit, globale Muster zu erkennen (Baum-Struktur)

**Analogie:**
- **Vorher:** Student lernt Vokabeln auswendig (schnell, aber kein Kontext)
- **Nachher:** Student lernt Grammatik (langsam, aber besseres Verst√§ndnis)

**Was das Modell lernen soll:**
- Nicht nur: "Oak_leaves kommen oft neben oak_wood vor" ‚úÖ (bereits gelernt)
- Sondern: "Oak_leaves bilden eine Krone OBEN auf einem vertikalen oak_wood Stamm" ‚ùå (fehlt noch)

### Erwartete Metriken

**Training-Verlauf:**

| Epoch | Loss | Accuracy | Erwartete Qualit√§t |
|-------|------|----------|-------------------|
| 0-20 | 3.0 ‚Üí 1.5 | 20% ‚Üí 40% | Chaotisch (Warmup) |
| 20-50 | 1.5 ‚Üí 0.8 | 40% ‚Üí 60% | Erste Muster |
| 50-100 | 0.8 ‚Üí 0.3 | 60% ‚Üí 75% | Klare Strukturen |
| 100-150 | 0.3 ‚Üí 0.1 | 75% ‚Üí 85% | Gute Koh√§renz |
| 150-200 | 0.1 ‚Üí 0.05 | 85% ‚Üí 90% | Sehr gut |
| 200-300 | 0.05 ‚Üí 0.02 | 90% ‚Üí 95% | **Exzellent** |

**Wichtig:** Loss wird **langsamer** fallen als vorher (das ist gewollt!).

### Test-Punkte

**Fr√ºhe Tests (Strukturbildung):**
- **Epoch 50:** Erste koh√§rente Strukturen?
- **Epoch 100:** Klare Baumst√§mme?

**Mittlere Tests (Koh√§renz):**
- **Epoch 150:** Verbundene Kronen?
- **Epoch 200:** Keine schwebenden Teile?

**Finale Tests (Qualit√§t):**
- **Epoch 250:** Production-ready?
- **Epoch 300:** Beste m√∂gliche Qualit√§t

### Wie zu Testen

```bash
# Auf RunPod (w√§hrend Training l√§uft)
# In separatem Terminal

# Test nach Epoch 50
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=50.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 5 \
  --sampling-steps 100

# Test nach Epoch 100
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=100.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 5 \
  --sampling-steps 100

# Test nach Epoch 150
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=150.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 5 \
  --sampling-steps 100
```

### Zus√§tzliche Optimierungen

**1. Mehr Sampling Steps w√§hrend Generation:**
```bash
--sampling-steps 200  # Statt 50, f√ºr bessere Qualit√§t
```

**2. Gr√∂√üere Strukturen testen:**
```bash
--size big  # 16√ó32√ó16 statt 16√ó16√ó16
--size huge  # 24√ó64√ó24
```

**3. Verschiedene Prompts:**
```bash
--prompt "tall oak tree"  # Betonung auf H√∂he
--prompt "oak tree with thick trunk"  # Betonung auf Stamm
--prompt "small oak tree with dense leaves"  # Klein & dicht
```

## Training Starten

```bash
# Auf RunPod
cd "model N"
git pull  # Neue config holen

# Neues Training starten (von vorne)
python3 scripts/train_discrete_diffusion.py

# ODER: Von bestehendem Checkpoint fortsetzen
python3 scripts/train_discrete_diffusion.py \
  --resume lightning_logs/version_X/checkpoints/epoch=XX.ckpt
```

**Wichtig:** 
- **Neues Training** empfohlen (alte LR war zu hoch)
- Training dauert **~30-40 Stunden** (300 Epochs statt 200)
- Teste zwischendurch (Epoch 50, 100, 150)

## Erwartete Ergebnisse

### Beste Szenario (95% Wahrscheinlichkeit)
- ‚úÖ Deutlich bessere r√§umliche Koh√§renz
- ‚úÖ Verbundene Strukturen (keine schwebenden Teile)
- ‚úÖ Klare Baumform (Stamm + Krone)
- ‚ö†Ô∏è Immer noch nicht perfekt (eventuell kleine Artefakte)

### Realistisches Szenario (70% Wahrscheinlichkeit)
- ‚úÖ Viel besser als jetzt
- ‚úÖ Erkennbare B√§ume
- ‚ö†Ô∏è Manche Strukturen noch etwas inkoh√§rent
- ‚ö†Ô∏è Kleine schwebende Teile m√∂glich

### Schlechtestes Szenario (5% Wahrscheinlichkeit)
- ‚ö†Ô∏è Nur marginale Verbesserung
- ‚ùå UNet-Architektur fundamental limitiert
- üí° Dann: Hierarchical Generation oder Post-Processing n√∂tig

## Alternativen falls nicht genug Verbesserung

**1. Learning Rate noch weiter reduzieren:**
```yaml
learning_rate: 0.00003  # Statt 0.00005
num_epochs: 400  # Noch l√§nger trainieren
```

**2. Attention auf allen Levels:**
```yaml
attention_levels: [0, 1, 2, 3]  # Statt nur [2, 3]
```

**3. Mehr Residual Blocks:**
```yaml
num_res_blocks: 3  # Statt 2
```

**4. Classifier-Free Guidance:**
- Braucht Code-√Ñnderungen
- Verst√§rkt Text-Conditioning
- Wie Stable Diffusion

**5. Hierarchical Generation:**
- Grobe Struktur zuerst (Stamm-Position)
- Details sp√§ter (Bl√§tter)
- Braucht 2-stufiges Modell

**6. Post-Processing:**
- Regel-basierte Cleanup
- Entferne schwebende Bl√∂cke
- Verbinde getrennte Strukturen
- Schnellste L√∂sung, aber nicht elegant

## Zeitplan

| Tag | Aktion | Dauer |
|-----|--------|-------|
| 1 | Training starten (Epoch 0-50) | ~6h |
| 2 | Test Epoch 50, weiter bis 100 | ~12h |
| 3 | Test Epoch 100, weiter bis 150 | ~12h |
| 4 | Test Epoch 150, weiter bis 200 | ~12h |
| 5 | Test Epoch 200, weiter bis 250 | ~12h |
| 6 | Test Epoch 250, weiter bis 300 | ~12h |
| 7 | Finale Tests, Dokumentation | ~4h |

**Total: ~70 Stunden** (mit Tests und Analysen)

## Erfolgsmetriken

**Quantitativ:**
- Loss < 0.05 (aktuell: 0.005 ‚úÖ)
- Accuracy > 90% (aktuell: 99.5% ‚úÖ)
- **Neu:** Connectivity Score > 0.8 (messen wie verbunden Strukturen sind)

**Qualitativ:**
- Keine schwebenden Teile
- Vertikale St√§mme erkennbar
- Bl√§tter bilden Krone oben
- Strukturen sehen aus wie B√§ume

**Vergleich:**
- Besser als continuous diffusion (chaotisch)
- Vergleichbar mit low-res Point-E/DreamFusion
- Nicht perfekt, aber nutzbar

## Fazit

**Diese Strategie hat h√∂chste Erfolgswahrscheinlichkeit:**
1. ‚úÖ Einfach umzusetzen (nur Config-√Ñnderung)
2. ‚úÖ Theoretisch fundiert (mehr Zeit f√ºr globale Muster)
3. ‚úÖ Kein Code-Refactoring n√∂tig
4. ‚úÖ Kann von bestehendem Checkpoint fortsetzen

**Wenn das nicht reicht:**
- Dann liegt es an UNet-Architektur
- Hierarchical Generation oder Transformer n√∂tig
- Aber erst diese Strategie testen!

---

**N√§chster Schritt:** Training mit neuer Config starten und nach Epoch 50 testen! üöÄ
