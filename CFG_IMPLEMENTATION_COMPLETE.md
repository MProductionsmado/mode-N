# Classifier-Free Guidance (CFG) - Implementation Complete! üéâ

## ‚úÖ Was wurde implementiert

### 1. **Training mit Conditioning Dropout**
- `src/training/trainer_discrete_diffusion.py`:
  - 10% der Trainingsschritte mit leerem Text-Embedding
  - Modell lernt sowohl conditional als auch unconditional generation
  - Erm√∂glicht CFG w√§hrend Generation

### 2. **CFG Generation**
- `src/models/discrete_diffusion_3d.py`:
  - Neue `generate()` Methode mit `guidance_scale` Parameter
  - Neue `p_sample_cfg()` Methode f√ºr CFG Sampling
  - Formula: `output = uncond + scale √ó (cond - uncond)`

### 3. **Config Updates**
- `config/config.yaml`:
  - `conditioning_dropout: 0.1` f√ºr Training
  - `guidance_scale: 3.0` als Default f√ºr Generation

### 4. **Generation Script**
- `scripts/generate_discrete_diffusion.py`:
  - Neues `--guidance-scale` Argument
  - Automatisch verwendet Config-Default wenn nicht angegeben

## üöÄ Wie zu nutzen

### Training (NEU von vorne)

```bash
# Auf RunPod
cd "model N"
git pull

# Training mit CFG
python3 scripts/train_discrete_diffusion.py

# WICHTIG: Training von Epoch 0!
# Altes Checkpoint (Epoch 67) ist INKOMPATIBEL (wurde ohne conditioning dropout trainiert)
```

**Training Logs zeigen jetzt:**
```
Initialized Discrete Diffusion Model
Number of block categories: 50 (oder 172 je nach Config)
Timesteps: 1000
Conditioning Dropout: 10.0% (CFG)  ‚Üê NEU!
```

### Generation mit CFG

```bash
# Test 1: No Guidance (wie vorher)
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=XX.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 5 \
  --guidance-scale 1.0

# Test 2: Balanced Guidance (EMPFOHLEN) ‚≠ê
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=XX.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 10 \
  --guidance-scale 3.0

# Test 3: Strong Guidance (sehr konsistent, wenig Variation)
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=XX.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 5 \
  --guidance-scale 7.5

# Test 4: Weak Guidance (viel Variation)
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_X/checkpoints/epoch=XX.ckpt \
  --prompt "oak tree" \
  --size normal \
  --num-samples 5 \
  --guidance-scale 1.5
```

## üéØ Guidance Scale Guide

| Scale | Effekt | Wann nutzen |
|-------|--------|-------------|
| **1.0** | Kein CFG, nur Noise | Maximale Variation (eventuell zu chaotisch) |
| **1.5-2.0** | Schwach | Viel Variation, Prompt hat wenig Einfluss |
| **3.0** ‚≠ê | Balanced | **EMPFOHLEN: Gute Balance** |
| **5.0** | Mittel-Stark | Text wichtiger, weniger Variation |
| **7.5** | Stark | Wie Stable Diffusion, sehr konsistent |
| **10.0+** | Sehr stark | Fast keine Variation (Overfitting auf Prompt) |

## üìä Erwartete Ergebnisse

### Nach Epoch 50-100 mit CFG:

**Scale 1.0 (No Guidance):**
```
"oak tree" seed=1 ‚Üí Irgendein Baum (oak, birch, spruce?)
"oak tree" seed=2 ‚Üí Komplett anderer Baum
"oak tree" seed=3 ‚Üí Eventuell gar kein Baum
‚Üí ZU VIEL Variation ‚ùå
```

**Scale 3.0 (Balanced - EMPFOHLEN):** ‚≠ê
```
"oak tree" seed=1 ‚Üí Kleiner dichter oak tree ‚úÖ
"oak tree" seed=2 ‚Üí Gro√üer lichter oak tree ‚úÖ
"oak tree" seed=3 ‚Üí Mittlerer buschiger oak tree ‚úÖ
‚Üí PERFEKTE Balance! üéØ
```

**Scale 7.5 (Strong):**
```
"oak tree" seed=1 ‚Üí Oak tree Variante A
"oak tree" seed=2 ‚Üí Oak tree Variante A (90% gleich)
"oak tree" seed=3 ‚Üí Oak tree Variante A (90% gleich)
‚Üí ZU WENIG Variation ‚ùå
```

## ‚ö†Ô∏è Wichtige Hinweise

### 1. **Altes Checkpoint NICHT kompatibel**
```bash
# ‚ùå FALSCH - Epoch 67 wurde OHNE conditioning dropout trainiert:
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint lightning_logs/version_11/checkpoints/epoch=67.ckpt \
  --guidance-scale 3.0
# ‚Üí Funktioniert, aber CFG wirkt NICHT (unconditional wurde nie gelernt!)

# ‚úÖ RICHTIG - Neues Training MIT conditioning dropout:
python3 scripts/train_discrete_diffusion.py  # Von Epoch 0
# Warte bis Epoch 50-100
# Dann teste mit --guidance-scale 3.0
```

### 2. **Training dauert ~50-100 Epochs**
- Epoch 0-20: Modell lernt Basics (mit und ohne Text)
- Epoch 20-50: CFG beginnt zu wirken
- Epoch 50-100: CFG voll funktionsf√§hig
- **Test fr√ºhestens ab Epoch 50!**

### 3. **Batch Size eventuell reduzieren**
- CFG braucht 2√ó Forward Pass (conditional + unconditional)
- Wenn OOM: Reduziere `batch_size` von 2 auf 1

## üî¨ Testing Strategie

### Phase 1: Training (Epoch 0-50)
```bash
# Einfach trainieren lassen
python3 scripts/train_discrete_diffusion.py
```

### Phase 2: Erste Tests (Epoch 50)
```bash
# Test ohne CFG (Baseline)
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint .../epoch=50.ckpt \
  --prompt "oak tree" \
  --guidance-scale 1.0 \
  --num-samples 5

# Test mit CFG
python3 scripts/generate_discrete_diffusion.py \
  --checkpoint .../epoch=50.ckpt \
  --prompt "oak tree" \
  --guidance-scale 3.0 \
  --num-samples 5

# Vergleich: Sind sie unterschiedlicher?
```

### Phase 3: Scale Tuning (Epoch 100)
```bash
# Teste verschiedene Scales
for scale in 1.5 2.0 3.0 5.0 7.5; do
  python3 scripts/generate_discrete_diffusion.py \
    --checkpoint .../epoch=100.ckpt \
    --prompt "oak tree" \
    --guidance-scale $scale \
    --num-samples 3
done

# Finde besten Scale f√ºr dein Use-Case
```

## üìà Erfolgsmetrik

**Vorher (ohne CFG):**
- 10 Generationen von "oak tree" ‚Üí 9 fast identisch ‚ùå

**Nachher (mit CFG scale=3.0):**
- 10 Generationen von "oak tree" ‚Üí 10 deutlich unterschiedlich ‚úÖ
- Aber alle erkennbar als "oak tree" ‚úÖ

## üé® Advanced: Prompt Engineering mit CFG

**Mit CFG kannst du jetzt kreativ werden:**

```bash
# Spezifische Variationen
python3 scripts/generate_discrete_diffusion.py \
  --prompt "small dense oak tree" \
  --guidance-scale 5.0  # H√∂her = "small dense" wichtiger

python3 scripts/generate_discrete_diffusion.py \
  --prompt "tall oak tree" \
  --guidance-scale 5.0

python3 scripts/generate_discrete_diffusion.py \
  --prompt "oak tree" \
  --guidance-scale 2.0  # Niedriger = mehr Variation

# Kombination mit Temperature
python3 scripts/generate_discrete_diffusion.py \
  --prompt "oak tree" \
  --guidance-scale 3.0 \
  --temperature 1.2 \
  --sample-mode multinomial
```

## üîß Troubleshooting

### Problem: CFG wirkt nicht (alle Generationen noch gleich)
**L√∂sung:** 
- Training noch nicht lange genug (< Epoch 50)
- Oder: Alte Checkpoint ohne conditioning dropout
- ‚Üí Warte bis Epoch 100 oder trainiere neu

### Problem: CUDA OOM w√§hrend Training
**L√∂sung:**
```yaml
# config/config.yaml
training:
  batch_size: 1  # Von 2 reduziert
```

### Problem: Generationen zu chaotisch
**L√∂sung:**
- Erh√∂he `guidance_scale` von 3.0 auf 5.0 oder 7.5
- Oder: Weiter trainieren (Epoch 150+)

### Problem: Generationen noch zu √§hnlich
**L√∂sung:**
- Reduziere `guidance_scale` von 3.0 auf 1.5 oder 2.0
- Oder: Erh√∂he `conditioning_dropout` von 0.1 auf 0.15

---

## üéâ Zusammenfassung

**CFG ist jetzt implementiert!**

‚úÖ Training mit 10% conditioning dropout
‚úÖ Generation mit guidance_scale Parameter
‚úÖ Config mit sinnvollen Defaults (scale=3.0)
‚úÖ Generation Script mit --guidance-scale Argument

**N√§chste Schritte:**
1. `git pull` auf RunPod
2. Training starten (von Epoch 0)
3. Ab Epoch 50: Teste verschiedene guidance_scales
4. Finde optimalen Scale (vermutlich 3.0-5.0)
5. Genie√üe vielf√§ltige Generationen! üå≥üé≤

**Training Zeit:** ~50-100 Epochs = ~20-40 Stunden
**Lohnt sich?** JA! Industrie-Standard f√ºr Diffusion Models! üöÄ
