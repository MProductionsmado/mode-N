# Configuration for Minecraft 3D Asset Generator

# Data Settings
data:
  input_dir: "out"
  output_dir: "data"
  processed_dir: "data/processed"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42

# Model Architecture
model:
  # Size configurations
  sizes:
    normal:
      dims: [16, 16, 16]
      latent_dim: 128
    big:
      dims: [16, 32, 16]
      latent_dim: 192
    huge:
      dims: [24, 64, 24]
      latent_dim: 256
  
  # Text encoder
  text_encoder:
    model_name: "all-MiniLM-L6-v2"  # Sentence-BERT model
    embedding_dim: 384
    
  # 3D UNet for Diffusion
  encoder:
    channels: [64, 128, 256, 512]  # Reduced from [128, 256, 512, 1024] to fit in 24GB VRAM
    kernel_size: 4
    stride: 2
    padding: 1
    
  # Diffusion parameters (DDPM)
  diffusion:
    num_timesteps: 1000  # Number of diffusion steps
    noise_schedule: "cosine"  # 'linear' or 'cosine' (cosine is better)
    beta_start: 0.0001  # For linear schedule
    beta_end: 0.02  # For linear schedule
    time_embed_dim: 256  # Time embedding dimension
    num_res_blocks: 3  # INCREASED: More residual blocks for better long-range dependencies (was 2)
    attention_levels: [2, 3]  # FALLBACK: Level 1 attention uses too much VRAM (was [1,2,3])
    dropout: 0.15  # INCREASED: More dropout for better generalization (was 0.1)
    sampling_steps: 50  # DDIM sampling steps (50 is fast, 1000 is slow but better)

# Training Settings
training:
  batch_size: 8  # REDUCED: Attention at level 1 needs much more VRAM (was 8)
  num_epochs: 400  # INCREASED: Even more epochs for fine details
  learning_rate: 0.00002  # FURTHER REDUCED: Much slower learning for structural consistency (was 0.00005)
  weight_decay: 0.00001
  
  # Optimizer
  optimizer: "adam"
  scheduler: "cosine"
  warmup_epochs: 10  # INCREASED: Gradual warmup for stable slow learning
  
  # Checkpointing
  save_every_n_epochs: 50
  checkpoint_dir: "models"
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 75  # INCREASED: More patience for slower learning (was 50)
    monitor: "val/loss"
    
  # Logging
  log_dir: "logs"
  log_every_n_steps: 50
  
# Generation Settings
generation:
  num_samples: 1
  temperature: 1.0
  guidance_scale: 1.0  # Classifier-free guidance weight
  
# Hardware
hardware:
  device: "cuda"  # cuda or cpu
  num_workers: 8  # Data loading workers (0 for main process only)
  precision: 16  # 16 for mixed precision, 32 for full precision
  num_gpus: 1  # 1 for single GPU, -1 for ALL GPUs, [0,1,2] for specific GPUs
  
# Block vocabulary (most common Minecraft blocks in nature assets)
# Note: mcschematic uses "_wood" not "_log" for tree trunks!
blocks:
  air: 0
  oak_leaves: 1  # Count: 513,807
  birch_leaves: 2  # Count: 122,651
  jungle_leaves: 3  # Count: 114,375
  spruce_leaves: 4  # Count: 103,554
  oak_wood: 5  # Count: 86,032
  snow: 6  # Count: 60,163
  tall_grass: 7  # Count: 58,464
  birch_wood: 8  # Count: 51,290
  light_gray_concrete: 9  # Count: 40,949
  spruce_wood: 10  # Count: 39,180
  yellow_stained_glass_pane: 11  # Count: 38,470
  sunflower: 12  # Count: 36,600
  dark_oak_leaves: 13  # Count: 31,928
  andesite: 14  # Count: 30,111
  vine: 15  # Count: 24,225
  yellow_wool: 16  # Count: 23,335
  spruce_fence: 17  # Count: 21,864
  yellow_glazed_terracotta: 18  # Count: 20,239
  green_stained_glass_pane: 19  # Count: 19,967
  yellow_concrete: 20  # Count: 19,942
