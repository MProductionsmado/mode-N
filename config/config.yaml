# Configuration for Minecraft 3D Asset Generator

# Data Settings
data:
  input_dir: "out"
  output_dir: "data"
  processed_dir: "data/processed"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42

# Model Architecture
model:
  # Size configurations
  sizes:
    normal:
      dims: [16, 16, 16]
      latent_dim: 128
    big:
      dims: [16, 32, 16]
      latent_dim: 192
    huge:
      dims: [24, 64, 24]
      latent_dim: 256
  
  # Text encoder
  text_encoder:
    model_name: "all-MiniLM-L6-v2"  # Sentence-BERT model
    embedding_dim: 384
    
  # 3D UNet for Diffusion
  encoder:
    channels: [64, 128, 256, 512]  # Reduced from [128, 256, 512, 1024] to fit in 24GB VRAM
    kernel_size: 4
    stride: 2
    padding: 1
    
  # Diffusion parameters (DDPM)
  diffusion:
    num_timesteps: 1000  # Number of diffusion steps
    noise_schedule: "cosine"  # 'linear' or 'cosine' (cosine is better)
    beta_start: 0.0001  # For linear schedule
    beta_end: 0.02  # For linear schedule
    time_embed_dim: 256  # Time embedding dimension
    num_res_blocks: 3  # INCREASED: More residual blocks for better long-range dependencies (was 2)
    attention_levels: [2, 3]  # FALLBACK: Level 1 attention uses too much VRAM (was [1,2,3])
    dropout: 0.15  # INCREASED: More dropout for better generalization (was 0.1)
    sampling_steps: 250  # DDIM sampling steps (50 is fast, 1000 is slow but better)

# Training Settings
training:
  batch_size: 8  # REDUCED: Attention at level 1 needs much more VRAM (was 8)
  num_epochs: 400  # INCREASED: Even more epochs for fine details
  learning_rate: 0.00001  # FURTHER REDUCED: Much slower learning for structural consistency (was 0.00005)
  weight_decay: 0.00001
  
  # Optimizer
  optimizer: "adam"
  scheduler: "cosine"
  warmup_epochs: 10  # INCREASED: Gradual warmup for stable slow learning
  
  # Checkpointing
  save_every_n_epochs: 50
  checkpoint_dir: "models"
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 75  # INCREASED: More patience for slower learning (was 50)
    monitor: "val/loss"
    
  # Logging
  log_dir: "logs"
  log_every_n_steps: 50
  
# Generation Settings
generation:
  num_samples: 1
  temperature: 1.0
  guidance_scale: 1.0  # Classifier-free guidance weight
  
# Hardware
hardware:
  device: "cuda"  # cuda or cpu
  num_workers: 8  # Data loading workers (0 for main process only)
  precision: 16  # 16 for mixed precision, 32 for full precision
  num_gpus: 1  # 1 for single GPU, -1 for ALL GPUs, [0,1,2] for specific GPUs
  
# Block vocabulary (most common Minecraft blocks in nature assets)
# Note: mcschematic uses "_wood" not "_log" for tree trunks!
blocks:
  air: 0
  oak_leaves: 1
  spruce_leaves: 2
  birch_leaves: 3
  jungle_leaves: 4
  dark_oak_leaves: 5
  acacia_leaves: 6
  azalea_leaves: 7
  flowering_azalea_leaves: 8
  cherry_leaves: 9
  mangrove_leaves: 10
  oak_log: 11
  spruce_log: 12
  birch_log: 13
  jungle_log: 14
  dark_oak_log: 15
  acacia_log: 16
  short_grass: 17
  fern: 18
  grass_block: 19
  moss_block: 20
  moss_carpet: 21
  dandelion: 22
  poppy: 23
  pink_tulip: 29
  oxeye_daisy: 30
  sunflower: 31
  peony: 32
  pink_petals: 33
  brown_mushroom: 34
  red_mushroom: 35
  brown_mushroom_block: 36
  stone: 37
  cobblestone: 38
  mossy_cobblestone: 39
  andesite: 40
  granite: 41
  diorite: 42
  dirt: 43
  coarse_dirt: 44
  snow: 45
  snow_block: 46
  ice: 47
  cactus: 48
