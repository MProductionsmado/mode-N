# Configuration for Minecraft 3D Asset Generator

# Data Settings
data:
  input_dir: "out"
  output_dir: "data"
  processed_dir: "data/processed"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42

# Model Architecture
model:
  # Size configurations
  sizes:
    normal:
      dims: [16, 16, 16]
      latent_dim: 128
    big:
      dims: [16, 32, 16]
      latent_dim: 192
    huge:
      dims: [24, 64, 24]
      latent_dim: 256
  
  # Text encoder
  text_encoder:
    model_name: "all-MiniLM-L6-v2"  # Sentence-BERT model
    embedding_dim: 384
    
  # 3D UNet for Diffusion
  encoder:
    channels: [64, 128, 256, 512]  # Reduced from [128, 256, 512, 1024] to fit in 24GB VRAM
    kernel_size: 4
    stride: 2
    padding: 1
    
  # Diffusion parameters (DDPM)
  diffusion:
    num_timesteps: 1000  # Number of diffusion steps
    noise_schedule: "cosine"  # 'linear' or 'cosine' (cosine is better)
    beta_start: 0.0001  # For linear schedule
    beta_end: 0.02  # For linear schedule
    time_embed_dim: 256  # Time embedding dimension
    num_res_blocks: 3  # INCREASED: More residual blocks for better long-range dependencies (was 2)
    attention_levels: [2, 3]  # FALLBACK: Level 1 attention uses too much VRAM (was [1,2,3])
    dropout: 0.15  # INCREASED: More dropout for better generalization (was 0.1)
    sampling_steps: 50  # DDIM sampling steps (50 is fast, 1000 is slow but better)

# Training Settings
training:
  batch_size: 4  # REDUCED: Attention at level 1 needs much more VRAM (was 8)
  num_epochs: 400  # INCREASED: Even more epochs for fine details
  learning_rate: 0.00002  # FURTHER REDUCED: Much slower learning for structural consistency (was 0.00005)
  weight_decay: 0.00001
  
  # Optimizer
  optimizer: "adam"
  scheduler: "cosine"
  warmup_epochs: 10  # INCREASED: Gradual warmup for stable slow learning
  
  # Checkpointing
  save_every_n_epochs: 50
  checkpoint_dir: "models"
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 75  # INCREASED: More patience for slower learning (was 50)
    monitor: "val/loss"
    
  # Logging
  log_dir: "logs"
  log_every_n_steps: 50
  
# Generation Settings
generation:
  num_samples: 1
  temperature: 1.0
  guidance_scale: 1.0  # Classifier-free guidance weight
  
# Hardware
hardware:
  device: "cuda"  # cuda or cpu
  num_workers: 8  # Data loading workers (0 for main process only)
  precision: 16  # 16 for mixed precision, 32 for full precision
  num_gpus: 1  # 1 for single GPU, -1 for ALL GPUs, [0,1,2] for specific GPUs
  
# Block vocabulary (most common Minecraft blocks in nature assets)
# Note: mcschematic uses "_wood" not "_log" for tree trunks!
blocks:
  air: 0
  oak_wood: 1
  oak_log: 2
  oak_leaves: 3
  birch_wood: 4
  birch_log: 5
  birch_leaves: 6
  spruce_wood: 7
  spruce_log: 8
  spruce_leaves: 9
  acacia_wood: 10
  acacia_log: 11
  acacia_leaves: 12
  dark_oak_wood: 13
  dark_oak_log: 14
  dark_oak_leaves: 15
  oak_planks: 16
  birch_planks: 17
  spruce_planks: 18
  grass_block: 19
  dirt: 20
  coarse_dirt: 21
  podzol: 22
  stone: 23
  cobblestone: 24
  mossy_cobblestone: 25
  vine: 26
  bee_nest: 27
  beehive: 28
  mushroom_stem: 29
  brown_mushroom_block: 30
  red_mushroom_block: 31
  yellow_stained_glass: 32
  yellow_stained_glass_pane: 33
  yellow_wool: 34
  yellow_glazed_terracotta: 35
  yellow_concrete: 36
  orange_stained_glass: 37
  orange_stained_glass_pane: 38
  orange_wool: 39
  orange_concrete: 40
  dandelion: 41
  azure_bluet: 42
  oxeye_daisy: 43
  barrier: 44
  # Add more as needed
